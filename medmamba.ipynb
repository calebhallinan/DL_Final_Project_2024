{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhisoley/DL_Final_Project_2024/blob/main/medmamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZTjTNG5uaeU"
      },
      "source": [
        "## DL Project - Reading in Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBmWDhfMuaeW"
      },
      "source": [
        "This is a script to read in the ACDC dataset. Download the dataset from this url:\n",
        "\n",
        "https://humanheart-project.creatis.insa-lyon.fr/database/#collection/637218c173e9f0047faa00fb\n",
        "\n",
        "Then edit the paths below to match where the training and testing data is (these folders should be located in the downloaded data). Currently extracting just 1 of the 7-9 images for each subject (note: the index is 2 bc index 0 didn't always have a gt with it), so this is something we can change if we need.\n",
        "\n",
        "Should see 200 training images and 100 testing images.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BrvCw2_u0Dz",
        "outputId": "1f090779-fd3c-4b6b-d37e-60004f82e752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAu4M73GuaeX"
      },
      "outputs": [],
      "source": [
        "# Edit these variables to match your setup\n",
        "dataset_path_training = '/content/drive/My Drive/python/Resources/training'\n",
        "\n",
        "dataset_path_testing = '/content/drive/My Drive/python/Resources/testing'\n",
        "\n",
        "# dataset_path_training = '/Users/calebhallinan/Desktop/jhu/classes/deep_learning/DL_Final_Project_2024/data/ACDC/training'\n",
        "# dataset_path_testing = '/Users/calebhallinan/Desktop/jhu/classes/deep_learning/DL_Final_Project_2024/data/ACDC/testing'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rx1oAnmyuaeY"
      },
      "outputs": [],
      "source": [
        "### Import packages\n",
        "\n",
        "import os\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import re\n",
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "### Functions to load in the data ###\n",
        "\n",
        "# Regular expression to extract the patient number and frame number from filenames\n",
        "filename_pattern = re.compile(r'patient(\\d+)_frame(\\d+)(_gt)?\\.nii\\.gz')\n",
        "\n",
        "# Function to get sorting key from the filename\n",
        "def get_sort_key(filepath):\n",
        "    match = filename_pattern.search(os.path.basename(filepath))\n",
        "    if match:\n",
        "        patient_num = int(match.group(1))\n",
        "        frame_num = int(match.group(2))\n",
        "        return (patient_num, frame_num)\n",
        "    else:\n",
        "        raise ValueError(f'Filename does not match expected pattern: {filepath}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7Si-_2DuaeY",
        "outputId": "d14906d8-243b-4d01-bd45-c2320238cbc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Images array shape: (200, 224, 224)\n",
            "Training Ground truths array shape: (200, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "### Read in training data ###\n",
        "\n",
        "# Lists to hold the file paths for images and ground truths\n",
        "image_file_paths_train = []\n",
        "ground_truth_file_paths_train = []\n",
        "\n",
        "# Walk through the directory and collect all relevant file paths\n",
        "for root, dirs, files in os.walk(dataset_path_training):\n",
        "    for file in files:\n",
        "        if 'frame' in file:\n",
        "            full_path = os.path.join(root, file)\n",
        "            if '_gt' in file:\n",
        "                ground_truth_file_paths_train.append(full_path)\n",
        "            else:\n",
        "                image_file_paths_train.append(full_path)\n",
        "\n",
        "# Sort the file paths to ensure alignment\n",
        "image_file_paths_train.sort(key=get_sort_key)\n",
        "ground_truth_file_paths_train.sort(key=get_sort_key)\n",
        "\n",
        "# Check to make sure each image has a corresponding ground truth\n",
        "assert len(image_file_paths_train) == len(ground_truth_file_paths_train)\n",
        "for img_path, gt_path in zip(image_file_paths_train, ground_truth_file_paths_train):\n",
        "    assert get_sort_key(img_path) == get_sort_key(gt_path), \"Mismatch between image and ground truth files\"\n",
        "\n",
        "# Load the images and ground truths into numpy arrays\n",
        "# using 2 index bc not all 0 index had a gt\n",
        "images_train = [resize(nib.load(path).get_fdata()[:,:,2], (224,224)) for path in image_file_paths_train]\n",
        "ground_truths_train = [resize(nib.load(path).get_fdata()[:,:,2], (224,224)) for path in ground_truth_file_paths_train]\n",
        "\n",
        "# Stack the arrays into 4D numpy arrays\n",
        "images_array_train = np.stack(images_train)\n",
        "ground_truths_array_train = np.stack(ground_truths_train)\n",
        "\n",
        "print(f'Training Images array shape: {images_array_train.shape}')\n",
        "print(f'Training Ground truths array shape: {ground_truths_array_train.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goULMJKzuaeZ",
        "outputId": "c26b14b5-353e-420b-e3bd-a3c34930dc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Images array shape: (100, 224, 224)\n",
            "Test Ground truths array shape: (100, 224, 224)\n"
          ]
        }
      ],
      "source": [
        "### Read in testing data ###\n",
        "\n",
        "# Lists to hold the file paths for images and ground truths\n",
        "image_file_paths_test = []\n",
        "ground_truth_file_paths_test = []\n",
        "\n",
        "# Walk through the directory and collect all relevant file paths\n",
        "for root, dirs, files in os.walk(dataset_path_testing):\n",
        "    for file in files:\n",
        "        if 'frame' in file:\n",
        "            full_path = os.path.join(root, file)\n",
        "            if '_gt' in file:\n",
        "                ground_truth_file_paths_test.append(full_path)\n",
        "            else:\n",
        "                image_file_paths_test.append(full_path)\n",
        "\n",
        "# Sort the file paths to ensure alignment\n",
        "image_file_paths_test.sort(key=get_sort_key)\n",
        "ground_truth_file_paths_test.sort(key=get_sort_key)\n",
        "\n",
        "# Check to make sure each image has a corresponding ground truth\n",
        "assert len(image_file_paths_test) == len(ground_truth_file_paths_test)\n",
        "for img_path, gt_path in zip(image_file_paths_test, ground_truth_file_paths_test):\n",
        "    assert get_sort_key(img_path) == get_sort_key(gt_path), \"Mismatch between image and ground truth files\"\n",
        "\n",
        "# Load the images and ground truths into numpy arrays\n",
        "# using 2 index bc not all 0 index had a gt\n",
        "images_test = [resize(nib.load(path).get_fdata()[:,:,2], (224,224)) for path in image_file_paths_test]\n",
        "ground_truths_test = [resize(nib.load(path).get_fdata()[:,:,2], (224,224)) for path in ground_truth_file_paths_test]\n",
        "\n",
        "# Stack the arrays into 4D numpy arrays\n",
        "images_array_test = np.stack(images_test)\n",
        "ground_truths_array_test = np.stack(ground_truths_test)\n",
        "\n",
        "print(f'Test Images array shape: {images_array_test.shape}')\n",
        "print(f'Test Ground truths array shape: {ground_truths_array_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNET"
      ],
      "metadata": {
        "id": "R1uLCgA-cYG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def unet_model(input_shape):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "\n",
        "    # Downscaling path\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    # Upscaling path\n",
        "    up5 = layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv4)\n",
        "    up5 = layers.concatenate([up5, conv3], axis=3)\n",
        "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
        "    conv5 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv5)\n",
        "    up6 = layers.concatenate([up6, conv2], axis=3)\n",
        "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
        "    conv6 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv6)\n",
        "    up7 = layers.concatenate([up7, conv1], axis=3)\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(up7)\n",
        "    conv7 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Define input shape\n",
        "input_shape = (224, 224, 1)\n",
        "\n",
        "# Create the model\n",
        "model = unet_model(input_shape)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Display model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSAiutnA6OaI",
        "outputId": "78d8255c-ffa7-444e-bc3f-d7fd6eeb5908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 1)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 224, 224, 64)         640       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 224, 224, 64)         36928     ['conv2d[0][0]']              \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 112, 112, 64)         0         ['conv2d_1[0][0]']            \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 112, 112, 128)        73856     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 112, 112, 128)        147584    ['conv2d_2[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 128)          0         ['conv2d_3[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 56, 56, 256)          295168    ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 56, 56, 256)          590080    ['conv2d_4[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPoolin  (None, 28, 28, 256)          0         ['conv2d_5[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 512)          1180160   ['max_pooling2d_2[0][0]']     \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 512)          2359808   ['conv2d_6[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTr  (None, 56, 56, 256)          524544    ['conv2d_7[0][0]']            \n",
            " anspose)                                                                                         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 56, 56, 512)          0         ['conv2d_transpose[0][0]',    \n",
            "                                                                     'conv2d_5[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 56, 56, 256)          1179904   ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 56, 56, 256)          590080    ['conv2d_8[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2D  (None, 112, 112, 128)        131200    ['conv2d_9[0][0]']            \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate  (None, 112, 112, 256)        0         ['conv2d_transpose_1[0][0]',  \n",
            " )                                                                   'conv2d_3[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)          (None, 112, 112, 128)        295040    ['concatenate_1[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)          (None, 112, 112, 128)        147584    ['conv2d_10[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2D  (None, 224, 224, 64)         32832     ['conv2d_11[0][0]']           \n",
            " Transpose)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 224, 224, 128)        0         ['conv2d_transpose_2[0][0]',  \n",
            " )                                                                   'conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)          (None, 224, 224, 64)         73792     ['concatenate_2[0][0]']       \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)          (None, 224, 224, 64)         36928     ['conv2d_12[0][0]']           \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)          (None, 224, 224, 1)          65        ['conv2d_13[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 7696193 (29.36 MB)\n",
            "Trainable params: 7696193 (29.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(images_array_train, ground_truths_array_train, batch_size=32, epochs=10, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(images_array_test, ground_truths_array_test)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geEc-4v06OdC",
        "outputId": "c38f4e2d-4ced-4f92-d325-6b4623500233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 67s 4s/step - loss: 1.4794 - accuracy: 0.8436 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6688 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 5s 967ms/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 5s 948ms/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 5s 1s/step - loss: 1.6678 - accuracy: 0.9432 - val_loss: 1.5380 - val_accuracy: 0.9386\n",
            "4/4 [==============================] - 4s 1s/step - loss: 1.6921 - accuracy: 0.9399\n",
            "Test Loss: 1.6920727491378784, Test Accuracy: 0.9398632645606995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unet++"
      ],
      "metadata": {
        "id": "rqgfLli8pHBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class UpConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpConvBlock, self).__init__()\n",
        "        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.conv_block = ConvBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, skip_connection):\n",
        "        x = self.upconv(x)\n",
        "        x = torch.cat([x, skip_connection], dim=1)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "class UNetPlusPlus(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(UNetPlusPlus, self).__init__()\n",
        "        self.conv1 = ConvBlock(in_channels, 64)\n",
        "        self.conv2 = ConvBlock(64, 128)\n",
        "        self.conv3 = ConvBlock(128, 256)\n",
        "        self.conv4 = ConvBlock(256, 512)\n",
        "        self.conv5 = ConvBlock(512, 1024)\n",
        "\n",
        "        self.upconv1 = UpConvBlock(1024, 512)\n",
        "        self.upconv2 = UpConvBlock(512, 256)\n",
        "        self.upconv3 = UpConvBlock(256, 128)\n",
        "        self.upconv4 = UpConvBlock(128, 64)\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        x1 = self.conv1(x)\n",
        "        x2 = F.max_pool2d(x1, kernel_size=2, stride=2)\n",
        "        x3 = self.conv2(x2)\n",
        "        x4 = F.max_pool2d(x3, kernel_size=2, stride=2)\n",
        "        x5 = self.conv3(x4)\n",
        "        x6 = F.max_pool2d(x5, kernel_size=2, stride=2)\n",
        "        x7 = self.conv4(x6)\n",
        "        x8 = F.max_pool2d(x7, kernel_size=2, stride=2)\n",
        "        x9 = self.conv5(x8)\n",
        "\n",
        "        # Decoder\n",
        "        x10 = self.upconv1(x9, x7)\n",
        "        x11 = self.upconv2(x10, x5)\n",
        "        x12 = self.upconv3(x11, x3)\n",
        "        x13 = self.upconv4(x12, x1)\n",
        "\n",
        "        # Final convolution\n",
        "        output = self.final_conv(x13)\n",
        "        return output\n",
        "\n",
        "# Example usage:\n",
        "input_channels = 1  # Example number of input channels (assuming grayscale images)\n",
        "num_classes = 4  # Example number of classes for the ACDC dataset\n",
        "model = UNetPlusPlus(input_channels, num_classes)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA_88JJD6Oif",
        "outputId": "b6dd6775-2bbf-4102-c71a-805b88c9cca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNetPlusPlus(\n",
            "  (conv1): ConvBlock(\n",
            "    (conv): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv2): ConvBlock(\n",
            "    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv3): ConvBlock(\n",
            "    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv4): ConvBlock(\n",
            "    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv5): ConvBlock(\n",
            "    (conv): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (upconv1): UpConvBlock(\n",
            "    (upconv): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (conv_block): ConvBlock(\n",
            "      (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv2): UpConvBlock(\n",
            "    (upconv): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (conv_block): ConvBlock(\n",
            "      (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv3): UpConvBlock(\n",
            "    (upconv): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (conv_block): ConvBlock(\n",
            "      (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (upconv4): UpConvBlock(\n",
            "    (upconv): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (conv_block): ConvBlock(\n",
            "      (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (final_conv): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "model = UNetPlusPlus(input_channels=1, num_classes=num_classes)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), 'unetplusplus_acdc.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QHuDUK0BpI_z",
        "outputId": "dfd89089-cbfb-4121-cf39-24afc2b86293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "UNetPlusPlus.__init__() got an unexpected keyword argument 'input_channels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-9b0d2883fd5d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNetPlusPlus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Define loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: UNetPlusPlus.__init__() got an unexpected keyword argument 'input_channels'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MedMamba"
      ],
      "metadata": {
        "id": "EV0KZGWmcayl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ConvSSMBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvSSMBlock, self).__init__()\n",
        "        self.conv_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.ssm_branch = nn.Sequential(\n",
        "            nn.LayerNorm((out_channels, 4, 4)),\n",
        "            nn.Linear(out_channels * 4 * 4, out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.merge = nn.Sequential(\n",
        "            nn.LayerNorm(out_channels),\n",
        "            nn.Linear(out_channels, out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv_branch(x)\n",
        "        ssm_out = self.ssm_branch(x.view(x.size(0), -1))\n",
        "        ssm_out = ssm_out.view(x.size(0), out_channels, 4, 4)  # Reshape to match conv_out\n",
        "        ssm_out = self.merge(ssm_out)\n",
        "        return conv_out + ssm_out  # Residual connection\n",
        "\n",
        "\n",
        "\n",
        "class MedMamba(nn.Module):\n",
        "    def __init__(self, in_channels=1, num_classes=4):\n",
        "        super(MedMamba, self).__init__()\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=4),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv_ssm_blocks = nn.Sequential(\n",
        "            ConvSSMBlock(64, 128),\n",
        "            ConvSSMBlock(128, 256),\n",
        "            ConvSSMBlock(256, 512),\n",
        "            ConvSSMBlock(512, 1024)\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.conv_ssm_blocks(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = MedMamba(in_channels=1, num_classes=4)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QV_kA1-caJQ",
        "outputId": "23e526c2-1ddd-4d45-abd8-1c0304d05252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedMamba(\n",
            "  (patch_embed): Sequential(\n",
            "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(4, 4))\n",
            "    (1): ReLU(inplace=True)\n",
            "  )\n",
            "  (conv_ssm_blocks): Sequential(\n",
            "    (0): ConvSSMBlock(\n",
            "      (conv_branch): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (ssm_branch): Sequential(\n",
            "        (0): LayerNorm((128, 4, 4), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=2048, out_features=128, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (merge): Sequential(\n",
            "        (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=128, out_features=128, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ConvSSMBlock(\n",
            "      (conv_branch): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (ssm_branch): Sequential(\n",
            "        (0): LayerNorm((256, 4, 4), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=4096, out_features=256, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (merge): Sequential(\n",
            "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (2): ConvSSMBlock(\n",
            "      (conv_branch): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (ssm_branch): Sequential(\n",
            "        (0): LayerNorm((512, 4, 4), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=8192, out_features=512, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (merge): Sequential(\n",
            "        (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (3): ConvSSMBlock(\n",
            "      (conv_branch): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (1): ReLU(inplace=True)\n",
            "      )\n",
            "      (ssm_branch): Sequential(\n",
            "        (0): LayerNorm((1024, 4, 4), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=16384, out_features=1024, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "      (merge): Sequential(\n",
            "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (2): ReLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=1024, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "train_images_tensor = torch.tensor(images_array_train).unsqueeze(1).float()  # Add channel dimension\n",
        "train_ground_truths_tensor = torch.tensor(ground_truths_array_train).unsqueeze(1).float()  # Add channel dimension\n",
        "\n",
        "test_images_tensor = torch.tensor(images_array_test).unsqueeze(1).float()  # Add channel dimension\n",
        "test_ground_truths_tensor = torch.tensor(ground_truths_array_test).unsqueeze(1).float()  # Add channel dimension\n",
        "\n",
        "# Create PyTorch datasets and dataloaders\n",
        "train_dataset = TensorDataset(train_images_tensor, train_ground_truths_tensor)\n",
        "test_dataset = TensorDataset(test_images_tensor, test_ground_truths_tensor)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, ground_truths in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, ground_truths)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "    with torch.no_grad():\n",
        "        for images, ground_truths in test_dataloader:\n",
        "            outputs = model(images)\n",
        "            all_predictions.append(outputs.numpy())\n",
        "            all_ground_truths.append(ground_truths.numpy())\n",
        "\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_ground_truths = np.concatenate(all_ground_truths)\n",
        "    accuracy = accuracy_score(all_ground_truths.flatten(), all_predictions.flatten())\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "a3wmKOiQcaMY",
        "outputId": "be5520f3-4722-4dfb-8686-5f3419104af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given normalized_shape=[128, 4, 4], expected input with shape [*, 128, 4, 4], but got input of size[32, 200704]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-677bb5b3b816>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-4b1feef7ba36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_ssm_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-4b1feef7ba36>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mconv_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mssm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssm_branch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mssm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to match conv_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mssm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    202\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2544\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         )\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[128, 4, 4], expected input with shape [*, 128, 4, 4], but got input of size[32, 200704]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another method https://github.com/YubiaoYue/MedMamba/blob/main/train.py"
      ],
      "metadata": {
        "id": "FjhmgbaTi7bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, datasets\n",
        "from tqdm import tqdm\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import re\n",
        "from skimage.transform import resize\n",
        "\n",
        "### Define the VSSM model ###\n",
        "class VSSM(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(VSSM, self).__init__()\n",
        "        # Define the architecture of the model\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 28 * 28)  # Flatten the input for the fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def main():\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"Using {} device.\".format(device))\n",
        "\n",
        "    num_classes = 2\n",
        "\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.tensor(images_array_train), torch.tensor(ground_truths_train))\n",
        "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(images_array_test), torch.tensor(ground_truths_test))\n",
        "\n",
        "    batch_size = 32\n",
        "    nw = min([os.cpu_count(), batch_size if batch_size > 1 else 0, 8])  # number of workers\n",
        "    print('Using {} dataloader workers every process'.format(nw))\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=nw)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=nw)\n",
        "\n",
        "    print(\"Using {} images for training, {} images for testing.\".format(len(train_dataset), len(test_dataset)))\n",
        "\n",
        "    net = VSSM(num_classes=num_classes)\n",
        "    net.to(device)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
        "\n",
        "    epochs = 100\n",
        "    best_acc = 0.0\n",
        "    save_path = './VSSMNet.pth'\n",
        "\n",
        "    train_steps = len(train_loader)\n",
        "    for epoch in range(epochs):\n",
        "        # train\n",
        "        net.train()\n",
        "        running_loss = 0.0\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, (images, labels) in enumerate(train_bar):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = net(images.unsqueeze(1).float())  # Add a channel dimension to images\n",
        "            loss = loss_function(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            train_bar.desc = \"Train Epoch [{}/{}] Loss: {:.3f}\".format(epoch + 1, epochs, loss.item())\n",
        "\n",
        "        # validate\n",
        "        net.eval()\n",
        "        acc = 0.0  # accumulate accurate number / epoch\n",
        "        with torch.no_grad():\n",
        "            test_bar = tqdm(test_loader, file=sys.stdout)\n",
        "            for images, labels in test_bar:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = net(images.unsqueeze(1).float())\n",
        "                predict_y = torch.argmax(outputs, dim=1)\n",
        "                acc += torch.eq(predict_y, labels.long()).sum().item()\n",
        "\n",
        "        val_accurate = acc / len(test_dataset)\n",
        "        print('[Epoch %d] Training Loss: %.3f  Test Accuracy: %.3f' %\n",
        "              (epoch + 1, running_loss / train_steps, val_accurate))\n",
        "\n",
        "        if val_accurate > best_acc:\n",
        "            best_acc = val_accurate\n",
        "            torch.save(net.state_dict(), save_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "28StWG8ZgNOa",
        "outputId": "5affd4ca-7663-4e3e-916e-5399e2a238c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-27b2c599cbea>:44: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  train_dataset = torch.utils.data.TensorDataset(torch.tensor(images_array_train), torch.tensor(ground_truths_train))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 2 dataloader workers every process\n",
            "Using 200 images for training, 100 images for testing.\n",
            "  0%|          | 0/7 [00:04<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-27b2c599cbea>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-27b2c599cbea>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add a channel dimension to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MambaUNET"
      ],
      "metadata": {
        "id": "wK-0niDGdSA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MambaBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters, kernel_size=(3, 3, 3), activation='relu', **kwargs):\n",
        "    super(MambaBlock, self).__init__(**kwargs)\n",
        "    self.filters = filters\n",
        "    self.kernel_size = kernel_size\n",
        "    self.activation = tf.keras.activations.get(activation)  # Map the activation string to the corresponding function\n",
        "\n",
        "\n",
        "    # Define layers for Mamba block\n",
        "    self.conv1 = tf.keras.layers.Conv3D(filters, kernel_size, padding='same')\n",
        "    self.conv2 = tf.keras.layers.Conv3D(filters, kernel_size, padding='same')\n",
        "    self.bnorm = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.activation(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.bnorm(x)\n",
        "    return x\n",
        "\n",
        "class MambaUNet(tf.keras.Model):\n",
        "    def __init__(self, input_shape, n_classes, filters=32):\n",
        "        super(MambaUNet, self).__init__()\n",
        "        self.inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "        # Encoder - Contracting Path (with Mamba Blocks)\n",
        "        conv1 = MambaBlock(filters, activation='relu')(self.inputs)\n",
        "        pool1 = tf.keras.layers.MaxPooling3D((2, 2, 2))(conv1)\n",
        "        conv2 = MambaBlock(filters * 2, activation='relu')(pool1)\n",
        "        pool2 = tf.keras.layers.MaxPooling3D((2, 2, 2))(conv2)\n",
        "        conv3 = MambaBlock(filters * 4, activation='relu')(pool2)\n",
        "        pool3 = tf.keras.layers.MaxPooling3D((2, 2, 2))(conv3)\n",
        "        conv4 = MambaBlock(filters * 8, activation='relu')(pool3)\n",
        "\n",
        "        # Decoder - Expanding Path (with Mamba Blocks and Skip Connections)\n",
        "        upconv5 = tf.keras.layers.Conv3DTranspose(filters * 4, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv4)\n",
        "        crop_concat5 = tf.keras.layers.Cropping3D(cropping=((0, 0), (0, 0), (0, 0)))(conv3)\n",
        "        if conv3.shape[3] < conv4.shape[3]:\n",
        "            conv3 = tf.pad(conv3, [[0, 0], [0, 0], [0, 0], [0, conv4.shape[3] - conv3.shape[3]], [0, 0]])\n",
        "        elif conv4.shape[3] < conv3.shape[3]:\n",
        "            conv4 = tf.pad(conv4, [[0, 0], [0, 0], [0, 0], [0, conv3.shape[3] - conv4.shape[3]], [0, 0]])\n",
        "\n",
        "        concat5 = tf.keras.layers.Concatenate(axis=-1)([upconv5, crop_concat5])\n",
        "        # concat5 = tf.keras.layers.Concatenate(axis=-1)([upconv5, crop_concat5])\n",
        "        conv5 = MambaBlock(filters * 4, activation='relu')(concat5)\n",
        "\n",
        "        upconv6 = tf.keras.layers.Conv3DTranspose(filters * 2, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv5)\n",
        "        crop_concat6 = tf.keras.layers.Cropping3D(cropping=((0, 0), (0, 0), (0, 0)))(conv2)\n",
        "        concat6 = tf.keras.layers.Concatenate()([upconv6, crop_concat6])\n",
        "        conv6 = MambaBlock(filters * 2, activation='relu')(concat6)\n",
        "\n",
        "        upconv7 = tf.keras.layers.Conv3DTranspose(filters, (2, 2, 2), strides=(2, 2, 2), padding='same')(conv6)\n",
        "        crop_concat7 = tf.keras.layers.Cropping3D(cropping=((0, 0), (0, 0), (0, 0)))(conv1)\n",
        "        concat7 = tf.keras.layers.Concatenate()([upconv7, crop_concat7])\n",
        "        conv7 = MambaBlock(filters, activation='relu')(concat7)\n",
        "\n",
        "        # Output layer\n",
        "        self.outputs = tf.keras.layers.Conv3D(n_classes, 1, activation='softmax')(conv7)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.outputs(inputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "rJpYGpCy-GTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "input_shape = (224, 224, 20, 1)  # Adjust dimensions and channels according to your data\n",
        "n_classes = 4  # Assuming 4 classes for segmentation\n",
        "model = MambaUNet(input_shape, n_classes)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images_array_train, ground_truths_array_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the data to match the model's input shape\n",
        "X_train = X_train.reshape(X_train.shape + (1,))\n",
        "X_val = X_val.reshape(X_val.shape + (1,))\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "X_test = images_array_test.reshape(images_array_test.shape + (1,))\n",
        "y_test = ground_truths_array_test\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "BDObP2jENzy9",
        "outputId": "9dd1f489-3764-4a85-e691-a21956f45950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 56, 56, 4, 128), (None, 56, 56, 5, 128)]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-75-6f622c1df30f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjust dimensions and channels according to your data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m  \u001b[0;31m# Assuming 4 classes for segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMambaUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-1465e08dece2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, n_classes, filters)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mconv4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconv4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mconcat5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupconv5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_concat5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;31m# concat5 = tf.keras.layers.Concatenate(axis=-1)([upconv5, crop_concat5])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mconv5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMambaBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/merging/concatenate.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 )\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dims\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 56, 56, 4, 128), (None, 56, 56, 5, 128)]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}