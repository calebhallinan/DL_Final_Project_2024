# -*- coding: utf-8 -*-
"""train_TransUnet.ipynb

Automatically generated by Colab.


"""

from google.colab import drive
drive.mount('/content/drive')

import os
import nibabel as nib
import numpy as np
import re
from skimage.transform import resize
import configparser
import pandas as pd


### Functions to load in the data ###

# Regular expression to extract the patient number and frame number from filenames
filename_pattern = re.compile(r'patient(\d+)_frame(\d+)(_gt)?\.nii\.gz')

# Function to get sorting key from the filename
def get_sort_key(filepath):
    match = filename_pattern.search(os.path.basename(filepath))
    if match:
        patient_num = int(match.group(1))
        frame_num = int(match.group(2))
        return (patient_num, frame_num)
    else:
        raise ValueError(f'Filename does not match expected pattern: {filepath}')

    # Function to extract the patient number and sort by it
def extract_patient_number(file_path):
    match = re.search(r"patient(\d+)", file_path)
    return int(match.group(1)) if match else None

# Import necessary libraries
import os
import numpy as np
import nibabel as nib
from skimage.transform import resize
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate
import matplotlib.pyplot as plt

# Set random seed for reproducibility
tf.random.set_seed(42)

# Edit these variables to match your setup
dataset_path_training = '/content/drive/My Drive/python/Resources/training'
dataset_path_testing = '/content/drive/My Drive/python/Resources/testing'


# Load and preprocess data
def load_data(file_paths):
    data = [resize(nib.load(path).get_fdata()[:, :, 2], (224, 224)) for path in file_paths]
    return np.expand_dims(data, axis=-1)

def load_labels(file_paths):
    labels = []
    for class_file in file_paths:
        config = pd.read_csv(class_file, sep=':', header=None)
        labels.append(config[config[0] == "Group"][1][2].strip())
        labels.append(config[config[0] == "Group"][1][2].strip())
    return labels

def calculate_dice(y_true, y_pred):
    intersection = np.sum(y_true * y_pred)
    union = np.sum(y_true) + np.sum(y_pred)
    return (2.0 * intersection) / (union + 1e-7)

# Read training data
image_file_paths_train = []
ground_truth_file_paths_train = []
class_file_paths_train = []

for root, dirs, files in os.walk(dataset_path_training):
    for file in files:
        if 'frame' in file:
            full_path = os.path.join(root, file)
            if '_gt' in file:
                ground_truth_file_paths_train.append(full_path)
            else:
                image_file_paths_train.append(full_path)
        if "Info" in file:
            class_file_paths_train.append(os.path.join(root, file))

image_file_paths_train.sort(key=get_sort_key)
ground_truth_file_paths_train.sort(key=get_sort_key)
class_file_paths_train = sorted(class_file_paths_train, key=extract_patient_number)

images_array_train = load_data(image_file_paths_train)
ground_truths_array_train = load_data(ground_truth_file_paths_train)
class_labels_train = load_labels(class_file_paths_train)

# Read testing data
image_file_paths_test = []
ground_truth_file_paths_test = []
class_file_paths_test = []

for root, dirs, files in os.walk(dataset_path_testing):
    for file in files:
        if 'frame' in file:
            full_path = os.path.join(root, file)
            if '_gt' in file:
                ground_truth_file_paths_test.append(full_path)
            else:
                image_file_paths_test.append(full_path)
        if "Info" in file:
            class_file_paths_test.append(os.path.join(root, file))

image_file_paths_test.sort(key=get_sort_key)
ground_truth_file_paths_test.sort(key=get_sort_key)
class_file_paths_test = sorted(class_file_paths_test, key=extract_patient_number)

images_array_test = load_data(image_file_paths_test)
ground_truths_array_test = load_data(ground_truth_file_paths_test)
class_labels_test = load_labels(class_file_paths_test)

# Normalize images
images_array_train = images_array_train / 255.0
images_array_test = images_array_test / 255.0

# Define TransUNet model
def TransUNet(input_shape, num_classes):
    inputs = Input(input_shape)

    # Encoder
    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)
    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)
    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

    # Bottleneck
    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)

    # Decoder
    up4 = UpSampling2D(size=(2, 2))(conv3)
    up4 = Conv2D(128, 2, activation='relu', padding='same')(up4)
    merge4 = concatenate([conv2, up4], axis=3)
    conv4 = Conv2D(128, 3, activation='relu', padding='same')(merge4)

    up5 = UpSampling2D(size=(2, 2))(conv4)
    up5 = Conv2D(64, 2, activation='relu', padding='same')(up5)
    merge5 = concatenate([conv1, up5], axis=3)
    conv5 = Conv2D(64, 3, activation='relu', padding='same')(merge5)

    # Output
    outputs = Conv2D(num_classes, 1, activation='sigmoid')(conv5)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Define TransUNet model
model = TransUNet(input_shape=(224, 224, 1), num_classes=4)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(images_array_train, ground_truths_array_train, validation_split=0.2, epochs=50, batch_size=16)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(images_array_test, ground_truths_array_test)

# Predict on test data
predictions = model.predict(images_array_test)

# Calculate Dice coefficient
dice_scores = []
for i in range(len(predictions)):
    dice = calculate_dice(ground_truths_array_test[i], predictions[i])
    dice_scores.append(dice)

average_dice = np.mean(dice_scores)
print("Average Dice coefficient:", average_dice)

import matplotlib.pyplot as plt

# Define a function to plot original images, ground truth masks, and predicted masks
def plot_samples(images, ground_truths, predictions, num_samples=5):
    plt.figure(figsize=(15, 5*num_samples))
    for i in range(num_samples):
        # Original Image
        plt.subplot(num_samples, 3, i*3+1)
        plt.imshow(images[i].squeeze(), cmap='gray')
        plt.title("Original Image")
        plt.axis('off')

        # Ground Truth Mask
        plt.subplot(num_samples, 3, i*3+2)
        plt.imshow(ground_truths[i].squeeze(), cmap='jet')
        plt.title("Ground Truth Mask")
        plt.axis('off')

        # Predicted Mask
        plt.subplot(num_samples, 3, i*3+3)
        plt.imshow(predictions[i].squeeze(), cmap='jet')
        plt.title("Predicted Mask")
        plt.axis('off')

    plt.tight_layout()
    plt.show()

# Plot the samples
plot_samples(images_array_test, ground_truths_array_test, predictions)

"""This code incorporates data augmentation during training using ImageDataGenerator, reduces the learning rate on plateau, and uses a generator to feed the augmented data to the model. These techniques should help speed up the training process and potentially improve the model's performance."""

# Import necessary libraries
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ReduceLROnPlateau

# Define data augmentation
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode='nearest'
)

# Define TransUNet model
model = TransUNet(input_shape=(224, 224, 1), num_classes=1)

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Define learning rate scheduler
lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)

# Train the model with data augmentation
history = model.fit(datagen.flow(images_array_train, ground_truths_array_train, batch_size=32),
                    steps_per_epoch=len(images_array_train) / 16,
                    validation_data=(images_array_test, ground_truths_array_test),
                    epochs=10,
                    callbacks=[lr_scheduler])

# Predict on test data
predictions = model.predict(images_array_test)

# Calculate Dice coefficient
dice_scores = []
for i in range(len(predictions)):
    dice = calculate_dice(ground_truths_array_test[i], predictions[i])
    dice_scores.append(dice)

average_dice = np.mean(dice_scores)
print("Average Dice coefficient:", average_dice)

# Visualize results (sample)
plot_samples(images_array_test, ground_truths_array_test, predictions)

